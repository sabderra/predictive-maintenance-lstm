{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for training an LSTM model to predict the Remaining Useful Life (RUL) of turbofan engines based on simulated sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set for reproducability\n",
    "np.random.seed(101)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Masking, Activation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "\n",
    "from data_generator import TSDataGenerator\n",
    "from util import set_log_dir, rmse\n",
    "from util import LRDecay\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath(\"./data/\")\n",
    "MODEL_DIR = os.path.abspath(\"./model/\")\n",
    "\n",
    "persist_run_stats = False # Enable for saving results to CouchDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this project is the NASA C-MAPSS Turbofan Engine Degradation Data Set https://ti.arc.nasa.gov/c/6/.  This data is model based simulated data from the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CMAPSSDATA.zip\t\t\t    RUL_FD004.txt    test_y.csv\r\n",
      "'Damage Propagation Modeling.pdf'   test_FD001.txt   train.csv\r\n",
      " readme.txt\t\t\t    test_FD002.txt   train_FD001.txt\r\n",
      " RUL_FD001.txt\t\t\t    test_FD003.txt   train_FD002.txt\r\n",
      " RUL_FD002.txt\t\t\t    test_FD004.txt   train_FD003.txt\r\n",
      " RUL_FD003.txt\t\t\t    test_x.csv\t     train_FD004.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is a multivariate time series. Each entry (row) reflects an operational cycle of a specific engine identified by engine id and cycle time. There are multiple entries per engine to represent different reporting times. Other columns represents different features 3 operational settings and 21 sensors:\n",
    "\n",
    "<pre>\n",
    "    1)      engine id\n",
    "    2)      time, in cycles\n",
    "    3)      operational setting 1\n",
    "    4)      operational setting 2\n",
    "    5)      operational setting 3\n",
    "    6)      sensor measurement  1\n",
    "    7)      sensor measurement  2\n",
    "    ...\n",
    "    26)     sensor measurement  21\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'cycle' ]\n",
    "\n",
    "# Three operational setting columns\n",
    "setting_cols = ['setting' + str(i) for i in range(1,4)]\n",
    "cols.extend(setting_cols)\n",
    "\n",
    "# Twenty one sensor columns\n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "cols.extend(sensor_cols)\n",
    "\n",
    "sort_cols = ['id','cycle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CMAPSS data is divided into training, test, and RUL data files. Each of these is further partitioned in 4 subsets that represents a different operational condition. The number of engines in each vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fn_id_map = {\n",
    "    \"train_FD001\": 1000,\n",
    "    \"train_FD002\": 2000,\n",
    "    \"train_FD003\": 3000,\n",
    "    \"train_FD004\": 4000,\n",
    "    \"test_FD001\":  5000,\n",
    "    \"test_FD002\":  6000,\n",
    "    \"test_FD003\":  7000,\n",
    "    \"test_FD004\":  8000,    \n",
    "    \"RUL_FD001\":  5000,\n",
    "    \"RUL_FD002\":  6000,\n",
    "    \"RUL_FD003\":  7000,\n",
    "    \"RUL_FD004\":  8000, \n",
    "}\n",
    "\n",
    "\n",
    "# Filename is mapped to a condition. Map:\n",
    "#       ONE (Sea Level) to 0\n",
    "#       SIX to 1\n",
    "fn_condition_map = {\n",
    "    \"train_FD001\": 1,\n",
    "    \"train_FD002\": 2,\n",
    "    \"train_FD003\": 1,\n",
    "    \"train_FD004\": 2,\n",
    "    \"test_FD001\":  1,\n",
    "    \"test_FD002\":  2,\n",
    "    \"test_FD003\":  1,\n",
    "    \"test_FD004\":  2,    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(paths, col_names, sort_cols):\n",
    "    # read data \n",
    "    df = pd.DataFrame()\n",
    "    for p in paths:\n",
    "        instance_df = pd.read_csv(p, sep=\" \", header=None)\n",
    "        instance_df.drop(instance_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "        instance_df.columns = col_names\n",
    "        instance_df['filename'] = os.path.splitext(os.path.basename(p))[0]\n",
    "        \n",
    "        df = pd.concat((df, instance_df), sort=False) \n",
    "\n",
    "    df['condition'] = df['filename'].apply( lambda f: fn_condition_map[f])\n",
    "    df['id'] = df['id'] + df['filename'].apply( lambda f: fn_id_map[f])\n",
    "    df.drop(['filename'], axis=1, inplace=True)\n",
    "    df = df.sort_values(sort_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training, validation, and test data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (160359, 27)\n",
      "Val  :  (47087, 27)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(DATA_DIR, \"train_FD*.txt\")\n",
    "all_files = glob.glob(path)\n",
    "\n",
    "train_df = load_data(all_files, cols, sort_cols)\n",
    "print(\"Train: \", train_df.shape)\n",
    "\n",
    "path1 = os.path.join(DATA_DIR, \"test_FD001.txt\")\n",
    "path2 = os.path.join(DATA_DIR, \"test_FD002.txt\")\n",
    "val_df = load_data([path1, path2], cols, sort_cols)\n",
    "print(\"Val  : \", val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0  1001      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
       "1  1001      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
       "2  1001      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
       "3  1001      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
       "4  1001      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
       "\n",
       "        s4     s5    ...          s13      s14     s15   s16  s17   s18  \\\n",
       "0  1400.60  14.62    ...      2388.02  8138.62  8.4195  0.03  392  2388   \n",
       "1  1403.14  14.62    ...      2388.07  8131.49  8.4318  0.03  392  2388   \n",
       "2  1404.20  14.62    ...      2388.03  8133.23  8.4178  0.03  390  2388   \n",
       "3  1401.87  14.62    ...      2388.08  8133.83  8.3682  0.03  392  2388   \n",
       "4  1406.22  14.62    ...      2388.04  8133.80  8.4294  0.03  393  2388   \n",
       "\n",
       "     s19    s20      s21  condition  \n",
       "0  100.0  39.06  23.4190          1  \n",
       "1  100.0  39.00  23.4236          1  \n",
       "2  100.0  38.95  23.3442          1  \n",
       "3  100.0  38.88  23.3739          1  \n",
       "4  100.0  38.90  23.4044          1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read ground truth RUL data to update the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.02</td>\n",
       "      <td>1585.29</td>\n",
       "      <td>1398.21</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8125.55</td>\n",
       "      <td>8.4052</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.86</td>\n",
       "      <td>23.3735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.71</td>\n",
       "      <td>1588.45</td>\n",
       "      <td>1395.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8139.62</td>\n",
       "      <td>8.3803</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.02</td>\n",
       "      <td>23.3916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1586.94</td>\n",
       "      <td>1401.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8130.10</td>\n",
       "      <td>8.4441</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.08</td>\n",
       "      <td>23.4166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.44</td>\n",
       "      <td>1584.12</td>\n",
       "      <td>1406.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>8132.90</td>\n",
       "      <td>8.3917</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.3737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.51</td>\n",
       "      <td>1587.19</td>\n",
       "      <td>1401.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8129.54</td>\n",
       "      <td>8.4031</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.99</td>\n",
       "      <td>23.4130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0  5001      1    0.0023    0.0003     100.0  518.67  643.02  1585.29   \n",
       "1  5001      2   -0.0027   -0.0003     100.0  518.67  641.71  1588.45   \n",
       "2  5001      3    0.0003    0.0001     100.0  518.67  642.46  1586.94   \n",
       "3  5001      4    0.0042    0.0000     100.0  518.67  642.44  1584.12   \n",
       "4  5001      5    0.0014    0.0000     100.0  518.67  642.51  1587.19   \n",
       "\n",
       "        s4     s5    ...          s13      s14     s15   s16  s17   s18  \\\n",
       "0  1398.21  14.62    ...      2388.03  8125.55  8.4052  0.03  392  2388   \n",
       "1  1395.42  14.62    ...      2388.06  8139.62  8.3803  0.03  393  2388   \n",
       "2  1401.34  14.62    ...      2388.03  8130.10  8.4441  0.03  393  2388   \n",
       "3  1406.42  14.62    ...      2388.05  8132.90  8.3917  0.03  391  2388   \n",
       "4  1401.92  14.62    ...      2388.03  8129.54  8.4031  0.03  390  2388   \n",
       "\n",
       "     s19    s20      s21  condition  \n",
       "0  100.0  38.86  23.3735          1  \n",
       "1  100.0  39.02  23.3916          1  \n",
       "2  100.0  39.08  23.4166          1  \n",
       "3  100.0  39.00  23.3737          1  \n",
       "4  100.0  38.99  23.4130          1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Preparation\n",
    "\n",
    "Two step process. First step is to calculate the Remaining Useful Life (RUL) that will be used as the label. The calculation different when using training or test files. Second, the data will be transformed using a min/max scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Training Data RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_training_rul(df):\n",
    "    # Data Labeling - generate column RUL\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left')\n",
    "    df['RUL'] = df['max'] - df['cycle']\n",
    "    df.drop('max', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>condition</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0  1001      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
       "1  1001      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
       "2  1001      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
       "3  1001      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
       "4  1001      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
       "\n",
       "        s4     s5 ...       s14     s15   s16  s17   s18    s19    s20  \\\n",
       "0  1400.60  14.62 ...   8138.62  8.4195  0.03  392  2388  100.0  39.06   \n",
       "1  1403.14  14.62 ...   8131.49  8.4318  0.03  392  2388  100.0  39.00   \n",
       "2  1404.20  14.62 ...   8133.23  8.4178  0.03  390  2388  100.0  38.95   \n",
       "3  1401.87  14.62 ...   8133.83  8.3682  0.03  392  2388  100.0  38.88   \n",
       "4  1406.22  14.62 ...   8133.80  8.4294  0.03  393  2388  100.0  38.90   \n",
       "\n",
       "       s21  condition  RUL  \n",
       "0  23.4190          1  191  \n",
       "1  23.4236          1  190  \n",
       "2  23.3442          1  189  \n",
       "3  23.3739          1  188  \n",
       "4  23.4044          1  187  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = calc_training_rul(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating RUL for validation and test data\n",
    "\n",
    "Calculating RUL for validation and test data is a bit more involved. We'll be using the data from the test_FD\\*.txt files and the corresponding RUL_FD_*.txt for the actual RUL ground truth. This allows us to utilize more data for training. The key calculation difference is that the last cycle in the these files must equal the ground truth RUL. Therefore when we compute the RUL for each entry by working backwards from the last cycle entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_rul_data(paths, col_names):\n",
    "    \n",
    "    # Filename is used to determine the condition\n",
    "    col_names.append('filename')\n",
    "\n",
    "    # read data \n",
    "    df = pd.DataFrame()\n",
    "    for p in paths:\n",
    "        instance_df = pd.read_csv(p, sep=\" \", header=None)\n",
    "        instance_df.drop(instance_df.columns[[1]], axis=1, inplace=True)\n",
    "        instance_df['filename'] = os.path.splitext(os.path.basename(p))[0]\n",
    "        instance_df = instance_df.reset_index()\n",
    "        instance_df.columns = col_names\n",
    "        \n",
    "        df = pd.concat((df, instance_df), sort=False) \n",
    "\n",
    "    df['id'] = df['id'] + df['filename'].apply( lambda f: fn_id_map[f]) + 1\n",
    "    df.drop(['filename'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359, 2)\n"
     ]
    }
   ],
   "source": [
    "path1 = os.path.join(DATA_DIR, \"RUL_FD001.txt\")\n",
    "path2 = os.path.join(DATA_DIR, \"RUL_FD002.txt\")\n",
    "val_rul_df = load_rul_data([path1, path2], ['id', 'RUL_actual'])\n",
    "print(val_rul_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_rul( feature_df, label_df):\n",
    "    # If index is not reset there will be int/str type issues when attempting the merge. \n",
    "    cycle_count_df = feature_df.groupby('id').count().reset_index()[['id','cycle']].rename(index=str, columns={\"cycle\":\"cycles\"}).reset_index(drop=True)\n",
    "    print(cycle_count_df.shape)\n",
    "\n",
    "    # Join cycle and RUL dataframes\n",
    "    assert cycle_count_df.shape[0] == label_df.shape[0]\n",
    "    tmp_df = cycle_count_df.merge(label_df, on=\"id\", how='left')\n",
    "\n",
    "    # The RUL actual column contains the value for the last cycle.\n",
    "    # Adding the cycles column will give us the RUL for the first cycle.\n",
    "    tmp_df['RUL_actual'] = tmp_df['cycles'] + tmp_df['RUL_actual']\n",
    "    tmp_df.drop('cycles',  axis=1, inplace=True)\n",
    "\n",
    "    # Join the two data frames\n",
    "    feature_df = feature_df.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "\n",
    "    # Use the cycle to decrement the RUL until the ground truth is reached.\n",
    "    feature_df['RUL'] = feature_df['RUL_actual'] - feature_df['cycle']\n",
    "    feature_df.drop('RUL_actual',  axis=1, inplace=True)\n",
    "    \n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>condition</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.02</td>\n",
       "      <td>1585.29</td>\n",
       "      <td>1398.21</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8125.55</td>\n",
       "      <td>8.4052</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.86</td>\n",
       "      <td>23.3735</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.71</td>\n",
       "      <td>1588.45</td>\n",
       "      <td>1395.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8139.62</td>\n",
       "      <td>8.3803</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.02</td>\n",
       "      <td>23.3916</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1586.94</td>\n",
       "      <td>1401.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8130.10</td>\n",
       "      <td>8.4441</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.08</td>\n",
       "      <td>23.4166</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.44</td>\n",
       "      <td>1584.12</td>\n",
       "      <td>1406.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8132.90</td>\n",
       "      <td>8.3917</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.3737</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.51</td>\n",
       "      <td>1587.19</td>\n",
       "      <td>1401.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8129.54</td>\n",
       "      <td>8.4031</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.99</td>\n",
       "      <td>23.4130</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0  5001      1    0.0023    0.0003     100.0  518.67  643.02  1585.29   \n",
       "1  5001      2   -0.0027   -0.0003     100.0  518.67  641.71  1588.45   \n",
       "2  5001      3    0.0003    0.0001     100.0  518.67  642.46  1586.94   \n",
       "3  5001      4    0.0042    0.0000     100.0  518.67  642.44  1584.12   \n",
       "4  5001      5    0.0014    0.0000     100.0  518.67  642.51  1587.19   \n",
       "\n",
       "        s4     s5 ...       s14     s15   s16  s17   s18    s19    s20  \\\n",
       "0  1398.21  14.62 ...   8125.55  8.4052  0.03  392  2388  100.0  38.86   \n",
       "1  1395.42  14.62 ...   8139.62  8.3803  0.03  393  2388  100.0  39.02   \n",
       "2  1401.34  14.62 ...   8130.10  8.4441  0.03  393  2388  100.0  39.08   \n",
       "3  1406.42  14.62 ...   8132.90  8.3917  0.03  391  2388  100.0  39.00   \n",
       "4  1401.92  14.62 ...   8129.54  8.4031  0.03  390  2388  100.0  38.99   \n",
       "\n",
       "       s21  condition  RUL  \n",
       "0  23.3735          1  142  \n",
       "1  23.3916          1  141  \n",
       "2  23.4166          1  140  \n",
       "3  23.3737          1  139  \n",
       "4  23.4130          1  138  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = calc_test_rul(val_df, val_rul_df)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All transforms will be done using a pipeline. At this time only the min_max scalar is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    # The default activation function for LSTM tanh, so we'll use a range of [-1,1].\n",
    "    ('min_max_scaler', preprocessing.MinMaxScaler(feature_range=(-1, 1)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saad/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>condition</th>\n",
       "      <th>RUL</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.999619</td>\n",
       "      <td>-0.999525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939980</td>\n",
       "      <td>0.854585</td>\n",
       "      <td>0.804223</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.819144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944164</td>\n",
       "      <td>0.940747</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>191</td>\n",
       "      <td>-1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.999495</td>\n",
       "      <td>-0.999288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.865915</td>\n",
       "      <td>0.816384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940128</td>\n",
       "      <td>0.941260</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>190</td>\n",
       "      <td>-0.99631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.999791</td>\n",
       "      <td>-0.997864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949649</td>\n",
       "      <td>0.845447</td>\n",
       "      <td>0.821459</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.936764</td>\n",
       "      <td>0.932408</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>189</td>\n",
       "      <td>-0.99262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.999553</td>\n",
       "      <td>-0.998576</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949649</td>\n",
       "      <td>0.817657</td>\n",
       "      <td>0.810304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.854394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.932055</td>\n",
       "      <td>0.935719</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>188</td>\n",
       "      <td>-0.98893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.999676</td>\n",
       "      <td>-0.999051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950014</td>\n",
       "      <td>0.817978</td>\n",
       "      <td>0.831131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.812341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933401</td>\n",
       "      <td>0.939119</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>187</td>\n",
       "      <td>-0.98524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
       "0  1001      1 -0.999619 -0.999525       1.0  1.0  0.939980  0.854585   \n",
       "1  1001      2 -0.999495 -0.999288       1.0  1.0  0.946000  0.865915   \n",
       "2  1001      3 -0.999791 -0.997864       1.0  1.0  0.949649  0.845447   \n",
       "3  1001      4 -0.999553 -0.998576       1.0  1.0  0.949649  0.817657   \n",
       "4  1001      5 -0.999676 -0.999051       1.0  1.0  0.950014  0.817978   \n",
       "\n",
       "         s4   s5     ...           s15  s16       s17  s18  s19       s20  \\\n",
       "0  0.804223  1.0     ...     -0.819144  1.0  0.836735  1.0  1.0  0.944164   \n",
       "1  0.816384  1.0     ...     -0.810692  1.0  0.836735  1.0  1.0  0.940128   \n",
       "2  0.821459  1.0     ...     -0.820312  1.0  0.795918  1.0  1.0  0.936764   \n",
       "3  0.810304  1.0     ...     -0.854394  1.0  0.836735  1.0  1.0  0.932055   \n",
       "4  0.831131  1.0     ...     -0.812341  1.0  0.857143  1.0  1.0  0.933401   \n",
       "\n",
       "        s21  condition  RUL  cycle_norm  \n",
       "0  0.940747       -1.0  191    -1.00000  \n",
       "1  0.941260       -1.0  190    -0.99631  \n",
       "2  0.932408       -1.0  189    -0.99262  \n",
       "3  0.935719       -1.0  188    -0.98893  \n",
       "4  0.939119       -1.0  187    -0.98524  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the columns that will be scaled\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "\n",
    "# Transform all columns except id, cycle, and RUL\n",
    "cols_transform = train_df.columns.difference(['id','cycle', 'RUL'])\n",
    "\n",
    "xform_train_df = pd.DataFrame(pipeline.fit_transform(train_df[cols_transform]), \n",
    "                             columns=cols_transform, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_transform)].join(xform_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>condition</th>\n",
       "      <th>RUL</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5001</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.999476</td>\n",
       "      <td>-0.997864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961872</td>\n",
       "      <td>0.831018</td>\n",
       "      <td>0.792780</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.930710</td>\n",
       "      <td>0.935674</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>142</td>\n",
       "      <td>-1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.999714</td>\n",
       "      <td>-0.999288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.937973</td>\n",
       "      <td>0.847905</td>\n",
       "      <td>0.779422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.846080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.941473</td>\n",
       "      <td>0.937692</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>141</td>\n",
       "      <td>-0.99631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5001</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.999572</td>\n",
       "      <td>-0.998338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951656</td>\n",
       "      <td>0.839835</td>\n",
       "      <td>0.807766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945510</td>\n",
       "      <td>0.940479</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>140</td>\n",
       "      <td>-0.99262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5001</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.999386</td>\n",
       "      <td>-0.998576</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951291</td>\n",
       "      <td>0.824765</td>\n",
       "      <td>0.832088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.838246</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940128</td>\n",
       "      <td>0.935697</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>139</td>\n",
       "      <td>-0.98893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5001</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.999519</td>\n",
       "      <td>-0.998576</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952568</td>\n",
       "      <td>0.841171</td>\n",
       "      <td>0.810543</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.830413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939455</td>\n",
       "      <td>0.940078</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>138</td>\n",
       "      <td>-0.98524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
       "0  5001      1 -0.999476 -0.997864       1.0  1.0  0.961872  0.831018   \n",
       "1  5001      2 -0.999714 -0.999288       1.0  1.0  0.937973  0.847905   \n",
       "2  5001      3 -0.999572 -0.998338       1.0  1.0  0.951656  0.839835   \n",
       "3  5001      4 -0.999386 -0.998576       1.0  1.0  0.951291  0.824765   \n",
       "4  5001      5 -0.999519 -0.998576       1.0  1.0  0.952568  0.841171   \n",
       "\n",
       "         s4   s5     ...           s15  s16       s17  s18  s19       s20  \\\n",
       "0  0.792780  1.0     ...     -0.828970  1.0  0.836735  1.0  1.0  0.930710   \n",
       "1  0.779422  1.0     ...     -0.846080  1.0  0.857143  1.0  1.0  0.941473   \n",
       "2  0.807766  1.0     ...     -0.802240  1.0  0.857143  1.0  1.0  0.945510   \n",
       "3  0.832088  1.0     ...     -0.838246  1.0  0.816327  1.0  1.0  0.940128   \n",
       "4  0.810543  1.0     ...     -0.830413  1.0  0.795918  1.0  1.0  0.939455   \n",
       "\n",
       "        s21  condition  RUL  cycle_norm  \n",
       "0  0.935674       -1.0  142    -1.00000  \n",
       "1  0.937692       -1.0  141    -0.99631  \n",
       "2  0.940479       -1.0  140    -0.99262  \n",
       "3  0.935697       -1.0  139    -0.98893  \n",
       "4  0.940078       -1.0  138    -0.98524  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['cycle_norm'] = val_df['cycle']\n",
    "xform_val_df = pd.DataFrame(pipeline.transform(val_df[cols_transform]), \n",
    "                            columns=cols_transform, \n",
    "                            index=val_df.index)\n",
    "val_join_df = val_df[val_df.columns.difference(cols_transform)].join(xform_val_df)\n",
    "val_df = val_join_df.reindex(columns = val_df.columns)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47087, 29)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the columns that will be used for features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the feature column list \n",
    "feature_cols = ['cycle_norm', 'condition']\n",
    "\n",
    "# Three operational setting columns\n",
    "setting_cols = ['setting' + str(i) for i in range(1,4)]\n",
    "feature_cols.extend(setting_cols)\n",
    "\n",
    "# Twenty one sensor columns\n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "feature_cols.extend(sensor_cols)\n",
    "\n",
    "# Build the label column list\n",
    "label_cols = ['RUL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Network\n",
    "The model is an LSTM network. The first layer is an LSTM layer with 128 units followed by a set of Dense layers with 64, 32, and 1 units respectively. \n",
    "\n",
    "[Keras LSTM](https://keras.io/layers/recurrent/) layers expect an input in the shape of a numpy array of 3 dimensions (samples, time steps, features) where samples is the number of training sequences, time steps is the look back window or sequence length and features is the number of features of each sequence at each time step. \n",
    "\n",
    "The LSTM layer is followed by three Dense layers with 64, 32 and finally 1 unit. All use a RELU activation function.\n",
    "\n",
    "An Adam optimizer is used and Root Mean Squared Error (RMSE) is used for the loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "    RMSE = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^i-RUL^i)^2 }\n",
    "\\end{equation}\n",
    "\n",
    "Mean Squared Error (MSE) and Mean Absolute Error (MAE) are also tracked as metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the series time window.\n",
    "sequence_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time series sequences that will be train on per batch.\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "num_features = len(feature_cols)\n",
    "num_out = len(label_cols)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, num_features),\n",
    "         units=128,\n",
    "         batch_input_shape=(batch_size, sequence_length, num_features),\n",
    "         stateful=False,\n",
    "         return_sequences=False))\n",
    "\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=num_out, activation='relu'))\n",
    "\n",
    "opt = Adam(lr=1e-3, decay=0.0, amsgrad=True)\n",
    "model.compile(loss=rmse, optimizer=opt, metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (128, 128)                79360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, 64)                 8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (128, 32)                 2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (128, 1)                  33        \n",
      "=================================================================\n",
      "Total params: 89,729\n",
      "Trainable params: 89,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log dir:  /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636\n",
      "Checkpoint path:  /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_pipeline.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup log directory\n",
    "log_dir, checkpoint_path = set_log_dir(MODEL_DIR, \"engine\")\n",
    "\n",
    "print(\"Log dir: \", log_dir)\n",
    "print(\"Checkpoint path: \", checkpoint_path)\n",
    "\n",
    "# Save the pipeline for later use\n",
    "from sklearn.externals import joblib \n",
    "joblib.dump(pipeline, log_dir+'/engine_pipeline.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run: 0, epoch: 0\n",
      "Number of items:  709\n",
      "Data shape:  (160359, 29)\n",
      "Max Iterations:  70316\n",
      "Max steps: 549 @ 128\n",
      "Number of items:  359\n",
      "Data shape:  (47087, 29)\n",
      "Max Iterations:  1494\n",
      "Max steps: 11 @ 128\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 106.9274 - mean_squared_error: 15042.6744 - mean_absolute_error: 95.4607 - val_loss: 77.6769 - val_mean_squared_error: 7029.2973 - val_mean_absolute_error: 67.4669\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 77.67687, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 68.8865 - mean_squared_error: 6139.9048 - mean_absolute_error: 60.8612 - val_loss: 59.4679 - val_mean_squared_error: 4070.0423 - val_mean_absolute_error: 52.9392\n",
      "\n",
      "Epoch 00002: val_loss improved from 77.67687 to 59.46793, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 67.3130 - mean_squared_error: 5970.4335 - mean_absolute_error: 59.4670 - val_loss: 56.4587 - val_mean_squared_error: 3730.4527 - val_mean_absolute_error: 47.3578\n",
      "\n",
      "Epoch 00003: val_loss improved from 59.46793 to 56.45869, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 58.3569 - mean_squared_error: 5126.8134 - mean_absolute_error: 52.1615 - val_loss: 45.6549 - val_mean_squared_error: 2759.5455 - val_mean_absolute_error: 41.1959\n",
      "\n",
      "Epoch 00004: val_loss improved from 56.45869 to 45.65486, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 48.4037 - mean_squared_error: 3844.4785 - mean_absolute_error: 44.1222 - val_loss: 44.9432 - val_mean_squared_error: 2253.3957 - val_mean_absolute_error: 41.6177\n",
      "\n",
      "Epoch 00005: val_loss improved from 45.65486 to 44.94321, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 53.6751 - mean_squared_error: 4504.7229 - mean_absolute_error: 49.3002 - val_loss: 34.6182 - val_mean_squared_error: 1880.2260 - val_mean_absolute_error: 32.1862\n",
      "\n",
      "Epoch 00006: val_loss improved from 44.94321 to 34.61820, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 51.9749 - mean_squared_error: 4312.8779 - mean_absolute_error: 47.8541 - val_loss: 49.0599 - val_mean_squared_error: 3268.7581 - val_mean_absolute_error: 41.8084\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 34.61820\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 48.5720 - mean_squared_error: 4164.7824 - mean_absolute_error: 44.3421 - val_loss: 45.0692 - val_mean_squared_error: 2422.7126 - val_mean_absolute_error: 41.4999\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 34.61820\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0001.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 45.5870 - mean_squared_error: 3313.4903 - mean_absolute_error: 41.6606 - val_loss: 41.3736 - val_mean_squared_error: 2037.0306 - val_mean_absolute_error: 36.9880\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 34.61820\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 46.4525 - mean_squared_error: 3910.0932 - mean_absolute_error: 42.2799 - val_loss: 30.1753 - val_mean_squared_error: 1092.5659 - val_mean_absolute_error: 27.2770\n",
      "\n",
      "Epoch 00010: val_loss improved from 34.61820 to 30.17532, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 44.4182 - mean_squared_error: 3388.3859 - mean_absolute_error: 40.2736 - val_loss: 27.0209 - val_mean_squared_error: 811.4241 - val_mean_absolute_error: 24.6330\n",
      "\n",
      "Epoch 00011: val_loss improved from 30.17532 to 27.02094, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 45.5663 - mean_squared_error: 3473.5952 - mean_absolute_error: 41.3648 - val_loss: 36.8871 - val_mean_squared_error: 1731.4143 - val_mean_absolute_error: 33.3750\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 27.02094\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 42.3883 - mean_squared_error: 2927.4981 - mean_absolute_error: 38.3205 - val_loss: 54.8414 - val_mean_squared_error: 3553.9486 - val_mean_absolute_error: 50.1690\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 27.02094\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 44.5991 - mean_squared_error: 3490.3354 - mean_absolute_error: 40.5795 - val_loss: 49.5937 - val_mean_squared_error: 2914.9983 - val_mean_absolute_error: 46.3832\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 27.02094\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 41.2671 - mean_squared_error: 2821.4384 - mean_absolute_error: 37.1206 - val_loss: 39.2569 - val_mean_squared_error: 1693.2930 - val_mean_absolute_error: 36.4794\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 27.02094\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 46.2915 - mean_squared_error: 3491.8770 - mean_absolute_error: 42.0600 - val_loss: 52.0839 - val_mean_squared_error: 3451.8476 - val_mean_absolute_error: 49.3631\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 27.02094\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 43.0105 - mean_squared_error: 3331.0177 - mean_absolute_error: 38.8747 - val_loss: 34.1319 - val_mean_squared_error: 1753.4766 - val_mean_absolute_error: 29.8976\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 27.02094\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 41.5364 - mean_squared_error: 2911.9678 - mean_absolute_error: 37.3659 - val_loss: 35.9974 - val_mean_squared_error: 1545.1059 - val_mean_absolute_error: 33.2148\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 27.02094\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 4.94e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 43.8548 - mean_squared_error: 3274.4934 - mean_absolute_error: 39.7270 - val_loss: 46.6241 - val_mean_squared_error: 2852.0468 - val_mean_absolute_error: 41.8753\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 27.02094\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 42.2790 - mean_squared_error: 3078.9874 - mean_absolute_error: 38.1728 - val_loss: 71.5410 - val_mean_squared_error: 7296.6176 - val_mean_absolute_error: 61.7913\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 27.02094\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 39.2783 - mean_squared_error: 2438.2539 - mean_absolute_error: 35.3279 - val_loss: 38.2892 - val_mean_squared_error: 1597.8913 - val_mean_absolute_error: 34.4221\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 27.02094\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 43.1720 - mean_squared_error: 3326.6968 - mean_absolute_error: 38.9073 - val_loss: 36.2373 - val_mean_squared_error: 1591.6255 - val_mean_absolute_error: 33.0163\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 27.02094\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 41.4980 - mean_squared_error: 2854.7096 - mean_absolute_error: 37.2147 - val_loss: 37.4732 - val_mean_squared_error: 1837.4107 - val_mean_absolute_error: 34.2241\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.02094\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 39.0248 - mean_squared_error: 2439.3844 - mean_absolute_error: 34.9572 - val_loss: 23.2063 - val_mean_squared_error: 635.2129 - val_mean_absolute_error: 20.5289\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.02094 to 23.20627, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 45.0446 - mean_squared_error: 3475.9201 - mean_absolute_error: 40.5577 - val_loss: 44.2888 - val_mean_squared_error: 2359.5271 - val_mean_absolute_error: 40.1474\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.20627\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 41.1274 - mean_squared_error: 2985.4235 - mean_absolute_error: 37.0763 - val_loss: 43.0414 - val_mean_squared_error: 2570.6631 - val_mean_absolute_error: 38.8448\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.20627\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 40.5524 - mean_squared_error: 2765.5742 - mean_absolute_error: 36.4746 - val_loss: 38.3918 - val_mean_squared_error: 1870.3774 - val_mean_absolute_error: 34.2184\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.20627\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 42.2700 - mean_squared_error: 3024.6324 - mean_absolute_error: 38.0712 - val_loss: 31.2499 - val_mean_squared_error: 1176.7861 - val_mean_absolute_error: 29.3195\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.20627\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 2.407e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 38.9223 - mean_squared_error: 2397.7894 - mean_absolute_error: 34.8719 - val_loss: 38.8681 - val_mean_squared_error: 2064.4081 - val_mean_absolute_error: 34.8888\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.20627\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.1239 - mean_squared_error: 2568.5345 - mean_absolute_error: 35.1425 - val_loss: 44.4939 - val_mean_squared_error: 2512.1946 - val_mean_absolute_error: 39.8670\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.20627\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 42.3335 - mean_squared_error: 3172.9171 - mean_absolute_error: 38.2579 - val_loss: 31.6998 - val_mean_squared_error: 1232.1116 - val_mean_absolute_error: 29.7802\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.20627\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 40.9439 - mean_squared_error: 2870.5608 - mean_absolute_error: 36.9535 - val_loss: 28.7826 - val_mean_squared_error: 890.1369 - val_mean_absolute_error: 25.4542\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.20627\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 65s 119ms/step - loss: 38.6622 - mean_squared_error: 2246.3487 - mean_absolute_error: 34.8221 - val_loss: 42.6212 - val_mean_squared_error: 2556.8595 - val_mean_absolute_error: 39.3320\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.20627\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 40.7748 - mean_squared_error: 2939.8038 - mean_absolute_error: 36.9757 - val_loss: 39.9360 - val_mean_squared_error: 1855.2869 - val_mean_absolute_error: 36.9499\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.20627\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 38.5884 - mean_squared_error: 2399.6702 - mean_absolute_error: 34.7315 - val_loss: 48.5481 - val_mean_squared_error: 3792.4889 - val_mean_absolute_error: 44.4655\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.20627\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 38.7345 - mean_squared_error: 2712.0075 - mean_absolute_error: 35.0818 - val_loss: 55.8799 - val_mean_squared_error: 4642.0950 - val_mean_absolute_error: 49.3420\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.20627\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.8077 - mean_squared_error: 2530.2065 - mean_absolute_error: 35.7747 - val_loss: 57.5006 - val_mean_squared_error: 3994.2332 - val_mean_absolute_error: 50.9602\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.20627\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 38.4816 - mean_squared_error: 2500.7667 - mean_absolute_error: 34.5014 - val_loss: 51.3043 - val_mean_squared_error: 2793.0845 - val_mean_absolute_error: 48.2381\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.20627\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 1.157e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 40.3311 - mean_squared_error: 2884.2518 - mean_absolute_error: 36.2297 - val_loss: 24.7209 - val_mean_squared_error: 794.7813 - val_mean_absolute_error: 21.6357\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.20627\n",
      "Epoch 00039: early stopping\n",
      "Loading previous best weights:  /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Training run: 1, epoch: 39\n",
      "Number of items:  709\n",
      "Data shape:  (160359, 29)\n",
      "Max Iterations:  70316\n",
      "Max steps: 549 @ 128\n",
      "Number of items:  359\n",
      "Data shape:  (47087, 29)\n",
      "Max Iterations:  1494\n",
      "Max steps: 11 @ 128\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 65s 117ms/step - loss: 40.9117 - mean_squared_error: 2776.9692 - mean_absolute_error: 36.9709 - val_loss: 56.9696 - val_mean_squared_error: 4420.2750 - val_mean_absolute_error: 53.8188\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.20627\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 1e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/549 [==============================] - 65s 118ms/step - loss: 39.6274 - mean_squared_error: 2816.9074 - mean_absolute_error: 35.5906 - val_loss: 23.1613 - val_mean_squared_error: 691.5747 - val_mean_absolute_error: 21.0098\n",
      "\n",
      "Epoch 00041: val_loss improved from 23.20627 to 23.16125, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 38.0891 - mean_squared_error: 2429.0735 - mean_absolute_error: 34.0993 - val_loss: 42.4718 - val_mean_squared_error: 2358.4898 - val_mean_absolute_error: 40.2177\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.16125\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 41.6124 - mean_squared_error: 2980.9995 - mean_absolute_error: 37.4148 - val_loss: 31.0514 - val_mean_squared_error: 1307.8546 - val_mean_absolute_error: 28.0858\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.16125\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 40.9207 - mean_squared_error: 2961.2610 - mean_absolute_error: 36.7712 - val_loss: 53.4307 - val_mean_squared_error: 3457.0345 - val_mean_absolute_error: 49.1078\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.16125\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 40.3256 - mean_squared_error: 2572.2218 - mean_absolute_error: 36.3135 - val_loss: 34.8043 - val_mean_squared_error: 1614.4398 - val_mean_absolute_error: 31.6191\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.16125\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 36.4079 - mean_squared_error: 2337.9820 - mean_absolute_error: 32.9445 - val_loss: 33.5945 - val_mean_squared_error: 1708.0348 - val_mean_absolute_error: 30.2593\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.16125\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 44.3177 - mean_squared_error: 3304.9628 - mean_absolute_error: 39.9184 - val_loss: 38.3625 - val_mean_squared_error: 1843.4342 - val_mean_absolute_error: 35.6606\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.16125\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 1e-05.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 38.8346 - mean_squared_error: 2667.9275 - mean_absolute_error: 34.9356 - val_loss: 36.2953 - val_mean_squared_error: 2120.8281 - val_mean_absolute_error: 32.5179\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.16125\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 38.8068 - mean_squared_error: 2475.7460 - mean_absolute_error: 34.7351 - val_loss: 30.9306 - val_mean_squared_error: 1262.9576 - val_mean_absolute_error: 27.7284\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.16125\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 41.2437 - mean_squared_error: 2870.4946 - mean_absolute_error: 37.1842 - val_loss: 57.7499 - val_mean_squared_error: 4823.2106 - val_mean_absolute_error: 52.6912\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.16125\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.1205 - mean_squared_error: 2608.2403 - mean_absolute_error: 34.9052 - val_loss: 27.8324 - val_mean_squared_error: 908.3196 - val_mean_absolute_error: 25.7317\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.16125\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 35.7480 - mean_squared_error: 2045.9214 - mean_absolute_error: 32.0813 - val_loss: 30.9147 - val_mean_squared_error: 1142.9347 - val_mean_absolute_error: 29.0788\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.16125\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 40.9320 - mean_squared_error: 2841.6721 - mean_absolute_error: 36.8635 - val_loss: 34.9994 - val_mean_squared_error: 1370.7973 - val_mean_absolute_error: 32.9867\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.16125\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 40.8190 - mean_squared_error: 2834.0086 - mean_absolute_error: 36.6416 - val_loss: 28.7479 - val_mean_squared_error: 1052.5792 - val_mean_absolute_error: 25.8291\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.16125\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.3557 - mean_squared_error: 2723.5471 - mean_absolute_error: 35.3923 - val_loss: 28.4448 - val_mean_squared_error: 1272.0360 - val_mean_absolute_error: 23.4369\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.16125\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 4.94e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.5412 - mean_squared_error: 2647.5217 - mean_absolute_error: 35.5076 - val_loss: 43.3534 - val_mean_squared_error: 2185.6746 - val_mean_absolute_error: 39.0899\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.16125\n",
      "Epoch 00056: early stopping\n",
      "Loading previous best weights:  /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Training run: 2, epoch: 56\n",
      "Number of items:  709\n",
      "Data shape:  (160359, 29)\n",
      "Max Iterations:  70316\n",
      "Max steps: 549 @ 128\n",
      "Number of items:  359\n",
      "Data shape:  (47087, 29)\n",
      "Max Iterations:  1494\n",
      "Max steps: 11 @ 128\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 42.3026 - mean_squared_error: 2916.5795 - mean_absolute_error: 38.3017 - val_loss: 43.8420 - val_mean_squared_error: 2283.2592 - val_mean_absolute_error: 41.2741\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.16125\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 37.2311 - mean_squared_error: 2278.5391 - mean_absolute_error: 33.3719 - val_loss: 36.2878 - val_mean_squared_error: 1591.3339 - val_mean_absolute_error: 31.7181\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.16125\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 65s 117ms/step - loss: 39.0281 - mean_squared_error: 2605.3368 - mean_absolute_error: 35.1086 - val_loss: 29.1566 - val_mean_squared_error: 1074.7929 - val_mean_absolute_error: 25.4492\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.16125\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 40.7182 - mean_squared_error: 2916.5763 - mean_absolute_error: 36.4635 - val_loss: 29.4880 - val_mean_squared_error: 1226.3147 - val_mean_absolute_error: 25.4324\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.16125\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 41.9903 - mean_squared_error: 3017.4391 - mean_absolute_error: 37.9459 - val_loss: 36.4884 - val_mean_squared_error: 1548.3020 - val_mean_absolute_error: 34.1287\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.16125\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 37.1191 - mean_squared_error: 2234.0271 - mean_absolute_error: 33.4253 - val_loss: 47.7945 - val_mean_squared_error: 2948.9280 - val_mean_absolute_error: 42.8974\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.16125\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 38.6829 - mean_squared_error: 2732.9127 - mean_absolute_error: 34.6927 - val_loss: 36.2790 - val_mean_squared_error: 1721.3478 - val_mean_absolute_error: 32.8478\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.16125\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 41.2901 - mean_squared_error: 2893.7771 - mean_absolute_error: 37.1052 - val_loss: 46.7210 - val_mean_squared_error: 3552.5656 - val_mean_absolute_error: 42.8112\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.16125\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 1e-06.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 37.5531 - mean_squared_error: 2346.7167 - mean_absolute_error: 33.7363 - val_loss: 27.4897 - val_mean_squared_error: 1030.9797 - val_mean_absolute_error: 24.5909\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.16125\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 41.3330 - mean_squared_error: 3036.0609 - mean_absolute_error: 37.0392 - val_loss: 20.0281 - val_mean_squared_error: 489.2286 - val_mean_absolute_error: 18.0189\n",
      "\n",
      "Epoch 00066: val_loss improved from 23.16125 to 20.02814, saving model to /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 38.8134 - mean_squared_error: 2574.3092 - mean_absolute_error: 34.9702 - val_loss: 42.0495 - val_mean_squared_error: 2690.9111 - val_mean_absolute_error: 36.5340\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 20.02814\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.1654 - mean_squared_error: 2572.5612 - mean_absolute_error: 35.1272 - val_loss: 52.5444 - val_mean_squared_error: 3382.8256 - val_mean_absolute_error: 46.8459\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 20.02814\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 42.3672 - mean_squared_error: 3094.8837 - mean_absolute_error: 38.1683 - val_loss: 53.1398 - val_mean_squared_error: 3160.3204 - val_mean_absolute_error: 49.6625\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 20.02814\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 38.7788 - mean_squared_error: 2559.6447 - mean_absolute_error: 34.9262 - val_loss: 46.8627 - val_mean_squared_error: 3301.9804 - val_mean_absolute_error: 44.0993\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 20.02814\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.4057 - mean_squared_error: 2828.6770 - mean_absolute_error: 35.3039 - val_loss: 47.8060 - val_mean_squared_error: 2632.0098 - val_mean_absolute_error: 44.4247\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 20.02814\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 38.9529 - mean_squared_error: 2460.8855 - mean_absolute_error: 35.2985 - val_loss: 38.3101 - val_mean_squared_error: 1794.3339 - val_mean_absolute_error: 34.5368\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 20.02814\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 41.1226 - mean_squared_error: 3069.5410 - mean_absolute_error: 37.0178 - val_loss: 36.9307 - val_mean_squared_error: 1678.9312 - val_mean_absolute_error: 34.7535\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 20.02814\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 39.1061 - mean_squared_error: 2493.0237 - mean_absolute_error: 35.1297 - val_loss: 38.9793 - val_mean_squared_error: 2152.4676 - val_mean_absolute_error: 35.5385\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 20.02814\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 4.9e-07.\n",
      "549/549 [==============================] - 64s 116ms/step - loss: 40.1559 - mean_squared_error: 2923.6995 - mean_absolute_error: 36.1111 - val_loss: 24.4791 - val_mean_squared_error: 756.7599 - val_mean_absolute_error: 21.8388\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 20.02814\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 2.4e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 38.2979 - mean_squared_error: 2478.5253 - mean_absolute_error: 34.5352 - val_loss: 39.2235 - val_mean_squared_error: 1878.8154 - val_mean_absolute_error: 37.3883\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 20.02814\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 2.4e-07.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 37.0694 - mean_squared_error: 2200.7921 - mean_absolute_error: 33.3570 - val_loss: 59.8546 - val_mean_squared_error: 3928.0637 - val_mean_absolute_error: 55.7592\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 20.02814\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 2.4e-07.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 41.5221 - mean_squared_error: 2987.9972 - mean_absolute_error: 37.3874 - val_loss: 46.3589 - val_mean_squared_error: 3724.8227 - val_mean_absolute_error: 43.8705\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 20.02814\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 2.4e-07.\n",
      "549/549 [==============================] - 65s 118ms/step - loss: 37.6375 - mean_squared_error: 2561.7919 - mean_absolute_error: 33.8447 - val_loss: 46.5985 - val_mean_squared_error: 2940.7323 - val_mean_absolute_error: 43.3054\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 20.02814\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 2.4e-07.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 39.6233 - mean_squared_error: 2626.8230 - mean_absolute_error: 35.4719 - val_loss: 35.6926 - val_mean_squared_error: 1597.3880 - val_mean_absolute_error: 33.0963\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 20.02814\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 2.4e-07.\n",
      "549/549 [==============================] - 64s 117ms/step - loss: 41.1148 - mean_squared_error: 2848.7294 - mean_absolute_error: 36.9341 - val_loss: 55.2269 - val_mean_squared_error: 3868.3059 - val_mean_absolute_error: 51.5705\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 20.02814\n",
      "Epoch 00081: early stopping\n",
      "Loading previous best weights:  /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n"
     ]
    }
   ],
   "source": [
    "training_runs = 3\n",
    "num_epochs = 100\n",
    "initial_epoch = 0\n",
    "epochs_before_decay = 10\n",
    "lrate = 1e-4\n",
    "patience = 15\n",
    "    \n",
    "tensorboard = TensorBoard(log_dir=log_dir,\n",
    "                        histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(checkpoint_path, verbose=1, save_best_only=True)\n",
    "\n",
    "epoch_loss_history = []\n",
    "epoch_val_history = []\n",
    "epoch_lr_history = []\n",
    "\n",
    "for tr_run in range(training_runs):\n",
    "\n",
    "    print(\"Training run: {}, epoch: {}\".format(tr_run, initial_epoch))\n",
    "    train_data_generator = TSDataGenerator(train_df, feature_cols, label_cols, batch_size=batch_size, num_steps=sequence_length, randomize=True, loop=True)\n",
    "    train_data_generator.print_summary()\n",
    "\n",
    "    val_data_generator = TSDataGenerator(val_df, feature_cols, label_cols, batch_size=batch_size, num_steps=sequence_length, randomize=True, loop=True)\n",
    "    val_data_generator.print_summary()\n",
    "\n",
    "    # Callbacks\n",
    "    lr_decay = LRDecay(initial_lrate=lrate, epochs_step=epochs_before_decay)\n",
    "    lr_scheduler = LearningRateScheduler(lr_decay.step_decay, verbose=1)\n",
    "    \n",
    "    earlystopper = EarlyStopping(patience=patience, verbose=1)\n",
    "    \n",
    "    callbacks = [ tensorboard, checkpointer, lr_scheduler, earlystopper]\n",
    "\n",
    "    # fit the network\n",
    "    history = model.fit_generator(\n",
    "        generator=train_data_generator.generate(), \n",
    "        validation_data=val_data_generator.generate(), \n",
    "        initial_epoch=initial_epoch,\n",
    "        epochs=num_epochs, \n",
    "        steps_per_epoch=train_data_generator.summary()['max_steps'],\n",
    "        validation_steps=val_data_generator.summary()['max_steps'],\n",
    "        shuffle=False,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks )\n",
    "    \n",
    "    # pick up after the last epoch\n",
    "    if len(history.epoch) > 0:        \n",
    "        initial_epoch = history.epoch[-1] + 1\n",
    "    \n",
    "    # TODO fix, sometimes Keras is returning an empty history dict.\n",
    "    try:\n",
    "        # Save loss/val metrics\n",
    "        epoch_loss_history += history.history['loss']\n",
    "        epoch_val_history += history.history['val_loss']\n",
    "        epoch_lr_history += lr_decay.history_lr\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # reduce starting lr as we iterate into another training loop\n",
    "    lrate /= 10\n",
    "    \n",
    "    print(\"Loading previous best weights: \", checkpoint_path)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss across all runs overlayed with the learning rate used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "\n",
    "epochs = np.arange(len(epoch_loss_history))\n",
    "ax1.plot(epochs, epoch_loss_history, label='loss')\n",
    "ax1.plot(epochs, epoch_val_history, label='val_loss')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"RMSE\")\n",
    "ax1.set_ylim(0)\n",
    "ax1.legend()\n",
    "\n",
    "# Add a rhs scale for the LR.\n",
    "ax2 = ax1.twinx()\n",
    "lr_epochs = []\n",
    "lr_steps = []\n",
    "\n",
    "for e1, l1, l2 in epoch_lr_history:\n",
    "    lr_epochs.append(e1)\n",
    "    lr_steps.append(l1)\n",
    "\n",
    "\n",
    "ax2.plot(lr_epochs, lr_steps, 'r.', label='lr')\n",
    "ax2.set_ylabel(\"Learning Rate\")\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test\n",
    "\n",
    "Loading and pre-processing the test data is similar to how validation data was handled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'FD003'\n",
    "\n",
    "test_X_path = os.path.join(DATA_DIR, 'test_' + dataset_name + '.txt')\n",
    "test_y_path = os.path.join(DATA_DIR, 'RUL_' + dataset_name + '.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>condition</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7001</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.94</td>\n",
       "      <td>1581.93</td>\n",
       "      <td>1396.93</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.48</td>\n",
       "      <td>8.3760</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.07</td>\n",
       "      <td>23.4468</td>\n",
       "      <td>1</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.02</td>\n",
       "      <td>1584.86</td>\n",
       "      <td>1398.90</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8137.44</td>\n",
       "      <td>8.4062</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.4807</td>\n",
       "      <td>1</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.68</td>\n",
       "      <td>1581.78</td>\n",
       "      <td>1391.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8138.25</td>\n",
       "      <td>8.3553</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.10</td>\n",
       "      <td>23.4244</td>\n",
       "      <td>1</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.20</td>\n",
       "      <td>1584.53</td>\n",
       "      <td>1395.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8137.07</td>\n",
       "      <td>8.3709</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.97</td>\n",
       "      <td>23.4782</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7001</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1589.03</td>\n",
       "      <td>1395.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8134.20</td>\n",
       "      <td>8.4146</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.09</td>\n",
       "      <td>23.3950</td>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0  7001      1   -0.0017   -0.0004     100.0  518.67  641.94  1581.93   \n",
       "1  7001      2    0.0006   -0.0002     100.0  518.67  642.02  1584.86   \n",
       "2  7001      3    0.0014   -0.0003     100.0  518.67  641.68  1581.78   \n",
       "3  7001      4    0.0027    0.0001     100.0  518.67  642.20  1584.53   \n",
       "4  7001      5   -0.0001    0.0001     100.0  518.67  642.46  1589.03   \n",
       "\n",
       "        s4     s5 ...       s14     s15   s16  s17   s18    s19    s20  \\\n",
       "0  1396.93  14.62 ...   8133.48  8.3760  0.03  391  2388  100.0  39.07   \n",
       "1  1398.90  14.62 ...   8137.44  8.4062  0.03  391  2388  100.0  39.04   \n",
       "2  1391.92  14.62 ...   8138.25  8.3553  0.03  391  2388  100.0  39.10   \n",
       "3  1395.34  14.62 ...   8137.07  8.3709  0.03  392  2388  100.0  38.97   \n",
       "4  1395.86  14.62 ...   8134.20  8.4146  0.03  391  2388  100.0  39.09   \n",
       "\n",
       "       s21  condition  RUL  \n",
       "0  23.4468          1  276  \n",
       "1  23.4807          1  275  \n",
       "2  23.4244          1  274  \n",
       "3  23.4782          1  273  \n",
       "4  23.3950          1  272  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the features\n",
    "test_df = load_data([test_X_path], cols, sort_cols)\n",
    "\n",
    "# Read in the labels (RUL)\n",
    "test_rul_df = load_rul_data([test_y_path], ['id', 'RUL_actual'])\n",
    "\n",
    "# Calculate the RUL and merge back to the test dataframe\n",
    "test_df = calc_test_rul(test_df, test_rul_df)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "\n",
    "norm_test_df = pd.DataFrame(pipeline.transform(test_df[cols_transform]), \n",
    "                            columns=cols_transform, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_transform)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cycle_norm</th>\n",
       "      <th>condition</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.999667</td>\n",
       "      <td>-0.999525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942169</td>\n",
       "      <td>0.813061</td>\n",
       "      <td>0.786651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923750</td>\n",
       "      <td>0.985947</td>\n",
       "      <td>0.284547</td>\n",
       "      <td>-0.849035</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944837</td>\n",
       "      <td>0.943846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.99631</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.999557</td>\n",
       "      <td>-0.999051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943629</td>\n",
       "      <td>0.828720</td>\n",
       "      <td>0.796084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923554</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>0.302228</td>\n",
       "      <td>-0.828283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942819</td>\n",
       "      <td>0.947625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.99262</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.999519</td>\n",
       "      <td>-0.999288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.937426</td>\n",
       "      <td>0.812260</td>\n",
       "      <td>0.762664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924776</td>\n",
       "      <td>0.985947</td>\n",
       "      <td>0.305845</td>\n",
       "      <td>-0.863258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946855</td>\n",
       "      <td>0.941349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.98893</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.999457</td>\n",
       "      <td>-0.998338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946912</td>\n",
       "      <td>0.826956</td>\n",
       "      <td>0.779039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926487</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>0.300576</td>\n",
       "      <td>-0.852539</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938110</td>\n",
       "      <td>0.947347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.98524</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.999591</td>\n",
       "      <td>-0.998338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951656</td>\n",
       "      <td>0.851005</td>\n",
       "      <td>0.781528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923848</td>\n",
       "      <td>0.986113</td>\n",
       "      <td>0.287762</td>\n",
       "      <td>-0.822511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946182</td>\n",
       "      <td>0.938071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cycle_norm  condition  setting1  setting2  setting3   s1        s2  \\\n",
       "0    -1.00000       -1.0 -0.999667 -0.999525       1.0  1.0  0.942169   \n",
       "1    -0.99631       -1.0 -0.999557 -0.999051       1.0  1.0  0.943629   \n",
       "2    -0.99262       -1.0 -0.999519 -0.999288       1.0  1.0  0.937426   \n",
       "3    -0.98893       -1.0 -0.999457 -0.998338       1.0  1.0  0.946912   \n",
       "4    -0.98524       -1.0 -0.999591 -0.998338       1.0  1.0  0.951656   \n",
       "\n",
       "         s3        s4   s5    ...          s12       s13       s14       s15  \\\n",
       "0  0.813061  0.786651  1.0    ...     0.923750  0.985947  0.284547 -0.849035   \n",
       "1  0.828720  0.796084  1.0    ...     0.923554  0.986333  0.302228 -0.828283   \n",
       "2  0.812260  0.762664  1.0    ...     0.924776  0.985947  0.305845 -0.863258   \n",
       "3  0.826956  0.779039  1.0    ...     0.926487  0.986058  0.300576 -0.852539   \n",
       "4  0.851005  0.781528  1.0    ...     0.923848  0.986113  0.287762 -0.822511   \n",
       "\n",
       "   s16       s17  s18  s19       s20       s21  \n",
       "0  1.0  0.816327  1.0  1.0  0.944837  0.943846  \n",
       "1  1.0  0.816327  1.0  1.0  0.942819  0.947625  \n",
       "2  1.0  0.816327  1.0  1.0  0.946855  0.941349  \n",
       "3  1.0  0.836735  1.0  1.0  0.938110  0.947347  \n",
       "4  1.0  0.816327  1.0  1.0  0.946182  0.938071  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RUL\n",
       "0  276\n",
       "1  275\n",
       "2  274\n",
       "3  273\n",
       "4  272"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()[label_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "Reload the latest, best model from this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:  /home/saad/workspaces/predictive-maintenance-lstm/model/engine20190106T1636/engine_20190106T1636.h5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (128, 128)                79360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, 64)                 8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (128, 32)                 2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (128, 1)                  33        \n",
      "=================================================================\n",
      "Total params: 89,729\n",
      "Trainable params: 89,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model: \", checkpoint_path)\n",
    "custom_objects={'rmse':rmse}\n",
    "inf_model = load_model(checkpoint_path, custom_objects=custom_objects)\n",
    "\n",
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Collect metrics across then entire test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:09,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items:  100\n",
      "Data shape:  (16596, 29)\n",
      "Max Iterations:  3896\n",
      "Max steps: 30 @ 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data_generator = TSDataGenerator(test_df, \n",
    "                                      feature_cols, \n",
    "                                      label_cols,\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_steps=sequence_length, \n",
    "                                      loop=False)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for p in tqdm(test_data_generator.generate()):\n",
    "    X.append(p[0])\n",
    "    y.append(p[1])\n",
    "\n",
    "test_X = np.vstack(X)\n",
    "test_y = np.vstack(y)\n",
    "\n",
    "test_data_generator.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11520/11520 [==============================] - 1s 59us/step\n",
      "Test score:\n",
      "\tRMSE: 59.91062597433726\n",
      "\tMSE: 6231.024659072028\n",
      "\tMAE: 54.945968797471785\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics are RMSE, MSE, MAE\n",
    "score = inf_model.evaluate(test_X, test_y, verbose=1, batch_size=batch_size)\n",
    "print('Test score:\\n\\tRMSE: {}\\n\\tMSE: {}\\n\\tMAE: {}'.format(*score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on one batch from the test data. This is a fragment of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_generator = TSDataGenerator(test_df, feature_cols, label_cols, batch_size=batch_size, num_steps=sequence_length, loop=False)\n",
    "\n",
    "g = test_data_generator.generate()\n",
    "test_X, test_y = next(g)\n",
    "y_pred_array = inf_model.predict_on_batch(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(rul_actual, rul_predicted):  \n",
    "    fig = plt.figure(figsize=(25,5))\n",
    "    cycles = np.arange(len(rul_actual))\n",
    "    plt.scatter(cycles, rul_predicted, marker='.', label=\"Predicted\")\n",
    "    plt.plot(cycles, rul_actual, 'r', label=\"Actual\")\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.xlim(0)\n",
    "    plt.ylabel(\"RUL\")\n",
    "    plt.ylim(0)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(test_y, y_pred_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the first batch which includes most of the first engine in the test file. As the sequence approaches the last time series entry, the prediction converged closeer to the actual RUL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Save parameters and results (requires couchdb and client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting training run.\n"
     ]
    }
   ],
   "source": [
    "if persist_run_stats:\n",
    "    print(\"Persisting training run.\")\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.append(os.path.abspath('../dl_config'))\n",
    "    from couchdb_helper import CouchDBHelper\n",
    "\n",
    "    username = os.getenv('COUCHDB_USER')\n",
    "    password = os.getenv('COUCHDB_PASSWORD')\n",
    "    \n",
    "    db_helper = CouchDBHelper(\"engine\", username, password)\n",
    "    db_helper.connect()\n",
    "\n",
    "    payload = {\n",
    "        'score': score,\n",
    "        'model': inf_model.to_json(),\n",
    "        'epoch_lr_history': epoch_lr_history,\n",
    "        'epoch_loss_history': epoch_loss_history,\n",
    "        'epoch_val_history': epoch_val_history\n",
    "    }\n",
    "\n",
    "    db_helper.save(os.path.basename(log_dir), payload)\n",
    "    db_helper.disconnect()\n",
    "else:\n",
    "    print(\"Not persisting training run.\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
